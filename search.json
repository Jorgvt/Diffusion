[
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html",
    "href": "00_Basic/00_ddpm_hugging_face.html",
    "title": "Annotated Diffusion from HF",
    "section": "",
    "text": "import math\nfrom inspect import isfunction\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom einops import rearrange, reduce\nfrom einops.layers.torch import Rearrange\n\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#what-is-a-diffusion-process",
    "href": "00_Basic/00_ddpm_hugging_face.html#what-is-a-diffusion-process",
    "title": "Annotated Diffusion from HF",
    "section": "What is a diffusion process?",
    "text": "What is a diffusion process?\n\nIn a few words: It’s a process where a neural network learns to gradually denoise data starting from pure noise.\n\nIt consists of 2 processes:\n\nA fixed or predefined forward diffusion process \\(q\\), that gradually adds Gaussian noise to an image until you end up with pure noise.\nA learned reverse diffusion process \\(p_\\theta\\), where a neural network is trained to gradually denoise an image starting from pure noise until you end with an actual image.\n\n\nurl = \"https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png\"\nresponse = requests.get(url)\nImage.open(BytesIO(response.content))\n\n\n\n\nThe forward and reverse processes happend for a predefined number of steps \\(T\\), and are indexed by \\(t\\), which indicates the step we’re in. Authors of DDPM use \\(T=1000\\).\nAt \\(t=0\\) we have the original image, and at each subsequent \\(t>0\\), it gets added some noise sampled from a Gaussian distribution. Given a sufficiently large \\(T\\) and a well behaved schedule for adding noise at each \\(t\\), you end up with an isotropic Gaussian distribution at \\(t=T\\).\n\nIsotropic means that the covariance matrix can be expressed as \\(C=\\sigma^2 I\\)"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#math",
    "href": "00_Basic/00_ddpm_hugging_face.html#math",
    "title": "Annotated Diffusion from HF",
    "section": "Math",
    "text": "Math\n\nMaybe I’ll add it or maybe not.\n\nThe important part as of now is that we are going to train a neural network that represents the reverse process \\(p_\\theta\\), and it will only have to predict the mean of the image \\(x_{t-1}\\) given \\(x_t\\) (later the formulation was improved to predict its variance as well, but we’ll keep it simple here).\nThe process, in simple steps will be:\n\nTake a random image sample from the dataset, \\(x_0\\).\nSample a random noise level between 1 and \\(T\\).\nSample pure Gaussian noise, \\(\\varepsilon\\).\nCorrupt the original image, \\(x_0\\) with the corresponding scaled noise according to the schedule (and \\(t\\)).\nThe NN predicts the added noise from the corrupted image.\n\nAs a NN we will be using a U-Net, so let’s define it."
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#the-network",
    "href": "00_Basic/00_ddpm_hugging_face.html#the-network",
    "title": "Annotated Diffusion from HF",
    "section": "The network",
    "text": "The network\n\ndef exists(x):\n    return x is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\n\ndef Upsample(dim, dim_out=None):\n    return nn.Sequential(\n        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n    )\n\n\ndef Downsample(dim, dim_out=None):\n    # No More Strided Convolutions or Pooling\n    return nn.Sequential(\n        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n    )\n\n\nAdding \\(t\\) knowledge through positional embeddings\nWe are going to be using the same network to denoise the image at different \\(t\\)’s, so we can encode \\(t\\)’s information into the network via positional embeddings. We will be using sinusoidal embeddings (inspired by the transformer).\n\nclass SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = math.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n\n### More building blocks of the network…\n\nclass WeightStandardizedConv2d(nn.Conv2d):\n    \"\"\"\n    https://arxiv.org/abs/1903.10520\n    weight standardization purportedly works synergistically with group normalization\n    \"\"\"\n\n    def forward(self, x):\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n\n        weight = self.weight\n        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n\n        return F.conv2d(\n            x,\n            normalized_weight,\n            self.bias,\n            self.stride,\n            self.padding,\n            self.dilation,\n            self.groups,\n        )\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, dim_out, groups=8):\n        super().__init__()\n        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n\n    def forward(self, x, scale_shift=None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.act(x)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n\n    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n        super().__init__()\n        self.mlp = (\n            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n            if exists(time_emb_dim)\n            else None\n        )\n\n        self.block1 = Block(dim, dim_out, groups=groups)\n        self.block2 = Block(dim_out, dim_out, groups=groups)\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n\n    def forward(self, x, time_emb=None):\n        scale_shift = None\n        if exists(self.mlp) and exists(time_emb):\n            time_emb = self.mlp(time_emb)\n            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n            scale_shift = time_emb.chunk(2, dim=1)\n\n        h = self.block1(x, scale_shift=scale_shift)\n        h = self.block2(h)\n        return h + self.res_conv(x)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim=1)\n        q, k, v = map(\n            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n        )\n        q = q * self.scale\n\n        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n        return self.to_out(out)\n\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n\n        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n                                    nn.GroupNorm(1, dim))\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim=1)\n        q, k, v = map(\n            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n        )\n\n        q = q.softmax(dim=-2)\n        k = k.softmax(dim=-1)\n\n        q = q * self.scale\n        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n\n        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n        return self.to_out(out)\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.GroupNorm(1, dim)\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.fn(x)\n\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        dim,\n        init_dim=None,\n        out_dim=None,\n        dim_mults=(1, 2, 4, 8),\n        channels=3,\n        self_condition=False,\n        resnet_block_groups=4,\n    ):\n        super().__init__()\n\n        # determine dimensions\n        self.channels = channels\n        self.self_condition = self_condition\n        input_channels = channels * (2 if self_condition else 1)\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n\n        # time embeddings\n        time_dim = dim * 4\n\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(dim),\n            nn.Linear(dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim),\n        )\n\n        # layers\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(\n                nn.ModuleList(\n                    [\n                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n                        Downsample(dim_in, dim_out)\n                        if not is_last\n                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n                    ]\n                )\n            )\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(\n                nn.ModuleList(\n                    [\n                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n                        Upsample(dim_out, dim_in)\n                        if not is_last\n                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n                    ]\n                )\n            )\n\n        self.out_dim = default(out_dim, channels)\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n\n    def forward(self, x, time, x_self_cond=None):\n        if self.self_condition:\n            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x_self_cond, x), dim=1)\n\n        x = self.init_conv(x)\n        r = x.clone()\n\n        t = self.time_mlp(time)\n\n        h = []\n\n        for block1, block2, attn, downsample in self.downs:\n            x = block1(x, t)\n            h.append(x)\n\n            x = block2(x, t)\n            x = attn(x)\n            h.append(x)\n\n            x = downsample(x)\n\n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t)\n\n        for block1, block2, attn, upsample in self.ups:\n            x = torch.cat((x, h.pop()), dim=1)\n            x = block1(x, t)\n\n            x = torch.cat((x, h.pop()), dim=1)\n            x = block2(x, t)\n            x = attn(x)\n\n            x = upsample(x)\n\n        x = torch.cat((x, r), dim=1)\n\n        x = self.final_res_block(x, t)\n        return self.final_conv(x)"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#defining-the-forward-diffusion-process",
    "href": "00_Basic/00_ddpm_hugging_face.html#defining-the-forward-diffusion-process",
    "title": "Annotated Diffusion from HF",
    "section": "Defining the forward diffusion process",
    "text": "Defining the forward diffusion process\n\nThe original DDPM autorhs employed a linear schedule increasing linearly from \\(\\beta_1=10^{-4}\\) and \\(\\beta_T=0.02\\). It was shown after that a cosine schedule would give better results.\n\nWe will define different schedules as of now and choose one later:\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"\n    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0.0001, 0.9999)\n\ndef linear_beta_schedule(timesteps):\n    beta_start = 0.0001\n    beta_end = 0.02\n    return torch.linspace(beta_start, beta_end, timesteps)\n\ndef quadratic_beta_schedule(timesteps):\n    beta_start = 0.0001\n    beta_end = 0.02\n    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n\ndef sigmoid_beta_schedule(timesteps):\n    beta_start = 0.0001\n    beta_end = 0.02\n    betas = torch.linspace(-6, 6, timesteps)\n    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n\nRegarding the \\(\\beta_t\\), and \\(\\alpha\\) parameters, they can be precomputed easily, so let’s do that to speed up the process:\n\nWe’ll assume a linear schedule.\n\n\ntimesteps = 300\n\nbetas = linear_beta_schedule(timesteps)\nalphas = 1 - betas\nalphas_cumprod = torch.cumprod(alphas, axis=0)\nalphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0) # Last one is pure noise so it has a 0 variance\nsqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n\n\nfig, axes = plt.subplots(1,4, figsize=(20,4))\naxes[0].plot(betas)\naxes[0].set_title(\"Betas\")\naxes[1].plot(alphas)\naxes[1].set_title(\"Alphas\")\naxes[2].plot(alphas_cumprod)\naxes[2].set_title(\"Alphas Cumprod\")\naxes[3].plot(alphas_cumprod_prev)\naxes[3].set_title(\"Alphas Cumprod Prev\")\nplt.show()\n\n\n\n\nWe can pre-calculate derivated quantities as well:\n\n# calculations for diffusion q(x_t | x_{t-1}) and others\nsqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\nsqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n\n# calculations for posterior q(x_{t-1} | x_t, x_0)\nposterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\nAnd we’ll define a function to extract the appropriate values given a \\(t\\):\n\ndef extract(a, t, x_shape):\n    batch_size = t.shape[0]\n    out = a.gather(-1, t.cpu())\n    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#seting-up-the-data-transformations",
    "href": "00_Basic/00_ddpm_hugging_face.html#seting-up-the-data-transformations",
    "title": "Annotated Diffusion from HF",
    "section": "Seting up the data transformations",
    "text": "Seting up the data transformations\n\nWe’ll need to set up the necessary transformations to turn the images from PIL into Tensors. Note that we’re going to scale the images into the \\([-1,1]\\) range so that the reverse process starts from the standard normal prior. We’re going to use an image of cats to showcase the process.\n\n\nfrom torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n\n\nimage_size = 128\ntransform = Compose([\n    Resize(image_size),\n    CenterCrop(image_size),\n    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n    Lambda(lambda t: (t * 2) - 1),\n    \n])\n\nx_start = transform(image).unsqueeze(0)\nx_start.shape\n\ntorch.Size([1, 3, 128, 128])\n\n\nWe also have to define the reverse transform, that takes a Tensor and turns it back into a PIL.Image:\n\nimport numpy as np\n\nreverse_transform = Compose([\n     Lambda(lambda t: (t + 1) / 2),\n     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n     Lambda(lambda t: t * 255.),\n     Lambda(lambda t: t.numpy().astype(np.uint8)),\n     ToPILImage(),\n])\n\n\nreverse_transform(x_start.squeeze())"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#the-diffusion-process",
    "href": "00_Basic/00_ddpm_hugging_face.html#the-diffusion-process",
    "title": "Annotated Diffusion from HF",
    "section": "The diffusion process",
    "text": "The diffusion process\n\n# forward diffusion (using the nice property)\ndef q_sample(x_start, t, noise=None):\n    if noise is None:\n        noise = torch.randn_like(x_start)\n\n    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n    sqrt_one_minus_alphas_cumprod_t = extract(\n        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n    )\n\n    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n\n\ndef get_noisy_image(x_start, t):\n  # add noise\n  x_noisy = q_sample(x_start, t=t)\n\n  # turn back into PIL image\n  noisy_image = reverse_transform(x_noisy.squeeze())\n\n  return noisy_image\n\n\nimport matplotlib.pyplot as plt\n\n# use seed for reproducability\ntorch.manual_seed(0)\n\n# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\ndef plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n    if not isinstance(imgs[0], list):\n        # Make a 2d grid even if there's just 1 row\n        imgs = [imgs]\n\n    num_rows = len(imgs)\n    num_cols = len(imgs[0]) + with_orig\n    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n    for row_idx, row in enumerate(imgs):\n        row = [image] + row if with_orig else row\n        for col_idx, img in enumerate(row):\n            ax = axs[row_idx, col_idx]\n            ax.imshow(np.asarray(img), **imshow_kwargs)\n            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    if with_orig:\n        axs[0, 0].set(title='Original image')\n        axs[0, 0].title.set_size(8)\n    if row_title is not None:\n        for row_idx in range(num_rows):\n            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n\n    plt.tight_layout()\n\n\nplot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])\n\n\n\n\nNow that we’re able to generate the forward diffusion process, we can define the loss function:\n\ndef p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n    if noise is None:\n        noise = torch.randn_like(x_start)\n\n    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n    predicted_noise = denoise_model(x_noisy, t)\n\n    if loss_type == 'l1':\n        loss = F.l1_loss(noise, predicted_noise)\n    elif loss_type == 'l2':\n        loss = F.mse_loss(noise, predicted_noise)\n    elif loss_type == \"huber\":\n        loss = F.smooth_l1_loss(noise, predicted_noise)\n    else:\n        raise NotImplementedError()\n\n    return loss"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#getting-the-data",
    "href": "00_Basic/00_ddpm_hugging_face.html#getting-the-data",
    "title": "Annotated Diffusion from HF",
    "section": "Getting the data",
    "text": "Getting the data\n\nWe’ll be using HF datasets to load Fashion MNIST, a good small dataset to test our model.\n\n\nfrom datasets import load_dataset\n\n# load dataset from the hub\ndataset = load_dataset(\"fashion_mnist\")\nimage_size = 28\nchannels = 1\nbatch_size = 128\n\nFound cached dataset fashion_mnist (/Users/jorgvt/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\nWe’ll use the with_transform() method to apply a function on the fly to the whole dataset in order to prepare our data:\n\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n\n# define image transformations (e.g. using torchvision)\ntransform = Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda t: (t * 2) - 1)\n])\n\n# define function\ndef transforms(examples):\n   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n   del examples[\"image\"]\n\n   return examples\n\ntransformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n\n# create dataloader\ndataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n\n\nbatch = next(iter(dataloader))\nprint(batch.keys())\n\ndict_keys(['pixel_values'])\n\n\n\nplt.imshow(batch[\"pixel_values\"][0].squeeze())\nplt.show()\n\n\n\n\n\n@torch.no_grad()\ndef p_sample(model, x, t, t_index):\n    betas_t = extract(betas, t, x.shape)\n    sqrt_one_minus_alphas_cumprod_t = extract(\n        sqrt_one_minus_alphas_cumprod, t, x.shape\n    )\n    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n    \n    # Equation 11 in the paper\n    # Use our model (noise predictor) to predict the mean\n    model_mean = sqrt_recip_alphas_t * (\n        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n    )\n\n    if t_index == 0:\n        return model_mean\n    else:\n        posterior_variance_t = extract(posterior_variance, t, x.shape)\n        noise = torch.randn_like(x)\n        # Algorithm 2 line 4:\n        return model_mean + torch.sqrt(posterior_variance_t) * noise \n\n# Algorithm 2 (including returning all images)\n@torch.no_grad()\ndef p_sample_loop(model, shape):\n    device = next(model.parameters()).device\n\n    b = shape[0]\n    # start from pure noise (for each example in the batch)\n    img = torch.randn(shape, device=device)\n    imgs = []\n\n    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n        imgs.append(img.cpu().numpy())\n    return imgs\n\n@torch.no_grad()\ndef sample(model, image_size, batch_size=16, channels=3):\n    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#training",
    "href": "00_Basic/00_ddpm_hugging_face.html#training",
    "title": "Annotated Diffusion from HF",
    "section": "Training",
    "text": "Training\n\nfrom pathlib import Path\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\nresults_folder = Path(\"./results\")\nresults_folder.mkdir(exist_ok = True)\nsave_and_sample_every = 1000\n\n\nfrom torch.optim import Adam\n\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device {device}\")\n\nmodel = Unet(\n    dim=image_size,\n    channels=channels,\n    dim_mults=(1, 2, 4,)\n)\nmodel.to(device)\n\noptimizer = Adam(model.parameters(), lr=1e-3)\n\nUsing device mps\n\n\nLet’s train it!\n\nfrom torchvision.utils import save_image\n\nepochs = 6\n\nfor epoch in range(epochs):\n    for step, batch in enumerate(dataloader):\n      optimizer.zero_grad()\n\n      batch_size = batch[\"pixel_values\"].shape[0]\n      batch = batch[\"pixel_values\"].to(device)\n\n      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n\n      loss = p_losses(model, batch, t, loss_type=\"huber\")\n\n      if step % 100 == 0:\n        print(\"Loss:\", loss.item())\n\n      loss.backward()\n      optimizer.step()\n\n      # save generated images\n      if step != 0 and step % save_and_sample_every == 0:\n        milestone = step // save_and_sample_every\n        batches = num_to_groups(4, batch_size)\n        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n        all_images = torch.cat(all_images_list, dim=0)\n        all_images = (all_images + 1) * 0.5\n        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
  },
  {
    "objectID": "00_Basic/00_ddpm_hugging_face.html#sampling-from-the-model",
    "href": "00_Basic/00_ddpm_hugging_face.html#sampling-from-the-model",
    "title": "Annotated Diffusion from HF",
    "section": "Sampling from the model",
    "text": "Sampling from the model\n\n# sample 64 images\nsamples = sample(model, image_size=image_size, batch_size=64, channels=channels)\n\n# show a random one\nrandom_index = 5\nplt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
  }
]